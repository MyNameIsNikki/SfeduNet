import sys
import yaml
import pandas as pd
import dask.dataframe as dd
import joblib
import os
import logging
import requests
import random
import time
from io import StringIO
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import create_engine, text
import psycopg2
from prometheus_client import Counter, Histogram
import xgboost as xgb
from dask.distributed import Client
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import LabelEncoder
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import mlflow
import mlflow.xgboost
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import openpyxl
from dask import delayed

# –ú–µ—Ç—Ä–∏–∫–∏ Prometheus
migrations_total = Counter('etl_migrations_total', 'Total number of ETL migrations')
migration_duration = Histogram('etl_migration_duration_seconds', 'Duration of ETL migrations')
rows_migrated = Counter('etl_rows_migrated_total', 'Total rows migrated')

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
class SlackHandler(logging.Handler):
    def __init__(self, webhook_url):
        super().__init__()
        self.webhook_url = webhook_url

    def emit(self, record):
        if record.levelno >= logging.ERROR:
            msg = self.format(record)
            requests.post(self.webhook_url, json={"text": msg})

def get_logger():
    logger = logging.getLogger("ETL")
    logger.setLevel(logging.INFO)
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler("logs/etl.log")
    formatter = logging.Formatter("%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL", "")
    if slack_webhook:
        slack_handler = SlackHandler(slack_webhook)
        slack_handler.setFormatter(formatter)
        logger.addHandler(slack_handler)
    return logger

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö (–ø–æ—Ç–æ–∫–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥)
def extract_features(config, data_chunks=None):
    if data_chunks is not None:
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º Dask
        num_rows = data_chunks.shape[0].compute()
        num_cols = len(data_chunks.columns)
        text_cols = sum(data_chunks.dtypes.apply(lambda x: x == "object").compute())
        numeric_cols = sum(data_chunks.dtypes.apply(lambda x: x in [int, float]).compute())
        avg_row_size = data_chunks.memory_usage(deep=True).sum().compute() / num_rows if num_rows > 0 else 0
        total_amount = data_chunks['–°—É–º–º–∞ –í–¢'].sum().compute() if '–°—É–º–º–∞ –í–¢' in data_chunks.columns else 0
        total_quantity = data_chunks['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].sum().compute() if '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ' in data_chunks.columns else 0
    else:
        engine = create_engine(config['source_db'])
        with engine.connect() as conn:
            num_cols = conn.execute(text(f"""
                SELECT COUNT(*) FROM information_schema.columns
                WHERE table_name = '{config['table']}'
            """)).scalar()
            num_rows = conn.execute(text(f"SELECT COUNT(*) FROM {config['table']}")).scalar()
            col_types = conn.execute(text(f"""
                SELECT data_type, COUNT(*) as count
                FROM information_schema.columns
                WHERE table_name = '{config['table']}'
                GROUP BY data_type
            """)).fetchall()
            type_dict = {row['data_type']: row['count'] for row in col_types}
            avg_row_size = conn.execute(text(f"""
                SELECT AVG(pg_column_size(t.*)) as avg_size
                FROM {config['table']} t
                LIMIT 1000
            """)).scalar() or 0
            text_cols = type_dict.get("character varying", 0) + type_dict.get("text", 0)
            numeric_cols = type_dict.get("integer", 0) + type_dict.get("numeric", 0)
            total_amount = conn.execute(text(f"SELECT SUM(\"–°—É–º–º–∞ –í–¢\") FROM {config['table']}")).scalar() or 0
            total_quantity = conn.execute(text(f"SELECT SUM(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ\") FROM {config['table']}")).scalar() or 0

    return {
        "num_columns": num_cols,
        "num_rows": num_rows,
        "source_engine": config['source_db'].split(":")[0] if 'source_db' in config else "unknown",
        "target_engine": config['target_db'].split(":")[0] if 'target_db' in config else "unknown",
        "text_columns": text_cols,
        "numeric_columns": numeric_cols,
        "avg_row_size": avg_row_size,
        "total_amount": total_amount,
        "total_quantity": total_quantity
    }

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø–æ—Ç–æ–∫–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥)
def preprocess_data(chunk):
    chunk = chunk.drop_duplicates()
    le = LabelEncoder()
    for col in chunk.select_dtypes(include=['object']).columns:
        chunk[col] = le.fit_transform(chunk[col].astype(str))
    return chunk

# –í–∞–ª–∏–¥–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏ (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫–∏)
def validate_migration(src_engine, tgt_engine, table, logger, sample_size=1000):
    with src_engine.connect() as src, tgt_engine.connect() as tgt:
        src_count = src.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
        tgt_count = tgt.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
        logger.info(f"üìä Validation: Source = {src_count}, Target = {tgt_count}")
        if src_count != tgt_count:
            logger.warning("‚ö†Ô∏è Row count mismatch.")
            return False
        src_sample = pd.read_sql(text(f"SELECT * FROM {table} ORDER BY RANDOM() LIMIT {sample_size}"), src)
        tgt_sample = pd.read_sql(text(f"SELECT * FROM {table} ORDER BY RANDOM() LIMIT {sample_size}"), tgt)
        if not src_sample.equals(tgt_sample):
            logger.warning("‚ö†Ô∏è Data integrity mismatch in sampled rows.")
            return False
        logger.info("‚úÖ Validation passed: row count and data integrity.")
        return True

# –ö–ª–∞—Å—Å –º–∏–≥—Ä–∞—Ü–∏–∏ (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
class ETLJob:
    def __init__(self, config, data_chunks=None, client=None):
        self.config = config
        self.data_chunks = data_chunks
        self.src_engine = create_engine(config['source_db'], pool_size=20, max_overflow=0) if self.data_chunks is None else None
        self.tgt_engine = create_engine(config['target_db'], pool_size=20, max_overflow=0)
        self.table = config['table']
        self.chunk_size = config['chunk_size']
        self.commit_interval = config['commit_interval']
        self.parallel_chunks = config.get('parallel_chunks', 1)
        self.logger = get_logger()
        self.total_rows = 0
        self.duration = 0
        self.client = client  # Dask Client –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

    def stream_data(self):
        if self.data_chunks is not None:
            # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å Dask
            for partition in self.data_chunks.to_delayed():
                chunk = partition.compute()
                chunk = preprocess_data(chunk)
                yield chunk
        else:
            with self.src_engine.connect().execution_options(stream_results=True) as conn:
                result = conn.execution_options(yield_per=self.chunk_size).execute(text(f"SELECT * FROM {self.table}"))
                while True:
                    chunk = result.fetchmany(self.chunk_size)
                    if not chunk:
                        break
                    df = pd.DataFrame(chunk, columns=result.keys())
                    yield df

    def _copy_to_postgres(self, df, conn):
        output = StringIO()
        df.to_csv(output, sep='\t', header=False, index=False)
        output.seek(0)
        cursor = conn.cursor()
        cursor.copy_from(output, self.table, sep='\t', null='')
        conn.commit()

    def _migrate_chunk(self, chunk, tgt_conn):
        try:
            with tgt_conn.connect() as conn:
                self._copy_to_postgres(chunk, conn.raw_connection())
                rows_migrated.inc(len(chunk))
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–∞: {e}")
            raise

    def run(self, parallel=False):
        self.logger.info(f"üöÄ Starting ETL job for table '{self.table}'")
        migrations_total.inc()
        start = time.time()
        buffer = []

        try:
            with self.tgt_engine.connect() as conn:
                conn.execute(text(f"DELETE FROM {self.table}"))
                self.logger.info(f"üßπ Target table '{self.table}' cleared.")

            with self.tgt_engine.begin() as tgt_conn:
                chunks = self.stream_data()
                if parallel and self.client:
                    # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è —Å Dask
                    tasks = [delayed(self._migrate_chunk)(chunk, self.tgt_engine) for chunk in chunks]
                    self.client.compute(tasks)
                    for chunk in chunks:
                        self.total_rows += len(chunk)
                elif parallel:
                    with ThreadPoolExecutor(max_workers=self.parallel_chunks) as executor:
                        for chunk in chunks:
                            buffer.append(chunk)
                            self.total_rows += len(chunk)
                            if self.total_rows % self.commit_interval < self.chunk_size:
                                executor.submit(self._migrate_chunk, pd.concat(buffer), tgt_conn)
                                buffer = []
                else:
                    for chunk in chunks:
                        buffer.append(chunk)
                        self.total_rows += len(chunk)
                        if self.total_rows % self.commit_interval < self.chunk_size:
                            self._copy_to_postgres(pd.concat(buffer), tgt_conn.raw_connection())
                            buffer = []

                if buffer:
                    self._copy_to_postgres(pd.concat(buffer), tgt_conn.raw_connection())

            self.duration = round(time.time() - start, 2)
            migration_duration.observe(self.duration)
            self.logger.info(f"‚úÖ ETL finished in {self.duration}s, {self.total_rows} rows migrated.")
            
            with self.tgt_engine.connect() as conn:
                conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_{self.table}_id ON {self.table} (id)"))
                self.logger.info(f"üìà Index added on {self.table}.id")

            if self.data_chunks is None:
                validate_migration(self.src_engine, self.tgt_engine, self.table, self.logger)

        except Exception as e:
            self.logger.error(f"‚ùå ETL failed: {e}")
            raise

# –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
def migrate_once(chunk_size, commit_interval, config, data_chunks=None, client=None):
    start = time.time()
    config['chunk_size'] = chunk_size
    config['commit_interval'] = commit_interval
    job = ETLJob(config, data_chunks, client)
    job.run(parallel=True)
    
    duration = time.time() - start
    data_loss = 0  # –£–ø—Ä–æ—â–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    total_amount = config.get('features', {}).get('total_amount', 0)
    fitness = (1.0 / (duration + 1)) * (1 - data_loss) * (1 + total_amount / 1e9)  # –£—á–∏—Ç—ã–≤–∞–µ–º –æ–±—ä—ë–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    
    feats = config.get('features', {})
    result = {
        "num_columns": feats.get("num_columns", 0),
        "num_rows": feats.get("num_rows", 0),
        "source_engine": feats.get("source_engine", ""),
        "target_engine": feats.get("target_engine", ""),
        "text_columns": feats.get("text_columns", 0),
        "numeric_columns": feats.get("numeric_columns", 0),
        "avg_row_size": feats.get("avg_row_size", 0),
        "total_amount": feats.get("total_amount", 0),
        "total_quantity": feats.get("total_quantity", 0),
        "best_batch": chunk_size,
        "best_commit": commit_interval,
        "duration": duration,
        "data_loss": data_loss,
    }
    df = pd.DataFrame([result])
    os.makedirs("ml_model", exist_ok=True)
    if os.path.exists("ml_model/dataset.csv"):
        existing_df = pd.read_csv("ml_model/dataset.csv")
        if set(existing_df.columns) == set(result.keys()):
            df.to_csv("ml_model/dataset.csv", mode='a', header=False, index=False)
        else:
            print("‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ dataset.csv –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª.")
            df.to_csv("ml_model/dataset.csv", mode='w', header=True, index=False)
    else:
        df.to_csv("ml_model/dataset.csv", mode='w', header=True, index=False)
    
    return fitness

def genetic_algorithm(config, data_chunks=None, client=None, pop_size=4, generations=2, initial_params=None):
    if initial_params:
        population = [[initial_params[0], initial_params[1]]]
        population.extend([[random.choice([50000, 100000, 200000]), random.choice([100000, 200000, 500000])] for _ in range(pop_size-1)])
    else:
        population = [[random.choice([50000, 100000, 200000]), random.choice([100000, 200000, 500000])] for _ in range(pop_size)]
    
    for gen in range(generations):
        scored = [(migrate_once(p[0], p[1], config, data_chunks, client), p) for p in population]
        scored.sort(reverse=True)
        best = scored[0][1]
        print(f"Gen {gen+1}: best={best}")
        elites = [p for _, p in scored[:pop_size//2]]
        children = []
        while len(children) < pop_size - len(elites):
            p1, p2 = random.sample(elites, 2)
            child = [
                random.choice([p1[0], p2[0]]),
                int((p1[1] + p2[1]) / 2 + random.randint(-50000, 50000))
            ]
            children.append(child)
        population = elites + children
    return best

# –û–±—É—á–µ–Ω–∏–µ ML-–º–æ–¥–µ–ª–∏ (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
def train_model(data_chunks=None):
    try:
        mlflow.set_experiment("etl_migration_optimization")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ MLflow: {e}. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–æ.")

    try:
        df = pd.read_csv("ml_model/dataset.csv")
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª dataset.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π—Ç–µ –µ–≥–æ —Å –ø–æ–º–æ—â—å—é –º–∏–≥—Ä–∞—Ü–∏–∏.")
        return

    # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if data_chunks is not None:
        sample = data_chunks.sample(frac=0.0001).compute()  # 0.01% –¥–∞–Ω–Ω—ã—Ö
        feats = extract_features({}, data_chunks)
        sample = pd.DataFrame([feats])
        df = pd.concat([df, sample], ignore_index=True)

    X = df.drop(columns=["best_batch", "best_commit", "duration", "data_loss"])
    y_batch = df["best_batch"]
    y_commit = df["best_commit"]

    with mlflow.start_run():
        model_batch = xgb.XGBRegressor(n_estimators=100, random_state=42)
        model_commit = xgb.XGBRegressor(n_estimators=100, random_state=42)

        X_train, X_test, yb_train, yb_test = train_test_split(X, y_batch, test_size=0.2, random_state=42)
        model_batch.fit(X_train, yb_train)
        batch_mae = mean_absolute_error(yb_test, model_batch.predict(X_test))
        print("Batch MAE:", batch_mae)

        X_train, X_test, yc_train, yc_test = train_test_split(X, y_commit, test_size=0.2, random_state=42)
        model_commit.fit(X_train, yc_train)
        commit_mae = mean_absolute_error(yc_test, model_commit.predict(X_test))
        print("Commit MAE:", commit_mae)

        mlflow.log_metric("batch_mae", batch_mae)
        mlflow.log_metric("commit_mae", commit_mae)
        mlflow.xgboost.log_model(model_batch, "model_batch")
        mlflow.xgboost.log_model(model_commit, "model_commit")

        os.makedirs("ml_model", exist_ok=True)
        joblib.dump(model_batch, "ml_model/model_batch.pkl")
        joblib.dump(model_commit, "ml_model/model_commit.pkl")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
def load_config(path='config.yaml'):
    try:
        with open(path, 'r') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        sys.exit(1)
    
    for key in config:
        if isinstance(config[key], str):
            config[key] = config[key].format(**os.environ)
    return config

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏
def run_etl(config, data_chunks=None, parallel=True, client=None):
    if config.get("optimize", False):
        feats = extract_features(config, data_chunks)
        config['features'] = feats
        feats_df = pd.DataFrame([feats])
        
        try:
            model_batch = joblib.load("ml_model/model_batch.pkl")
            model_commit = joblib.load("ml_model/model_commit.pkl")
        except FileNotFoundError:
            print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'python etl_project.py train' –¥–ª—è –∏—Ö —Å–æ–∑–¥–∞–Ω–∏—è.")
            sys.exit(1)
        
        pred_batch = int(model_batch.predict(feats_df)[0])
        pred_commit = int(model_commit.predict(feats_df)[0])
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ì–ê
        best_params = genetic_algorithm(config, data_chunks, client, initial_params=[pred_batch, pred_commit])
        config['chunk_size'] = best_params[0]
        config['commit_interval'] = best_params[1]
        print(f"ü§ñ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: batch = {best_params[0]}, commit = {best_params[1]}")

    job = ETLJob(config, data_chunks, client)
    job.run(parallel=parallel)
    return {"status": "success", "rows_migrated": job.total_rows, "duration": job.duration}

# REST API
app = FastAPI()

class MigrationRequest(BaseModel):
    config_path: str = "config.yaml"
    parallel: bool = True

@app.post("/migrate")
async def migrate(request: MigrationRequest):
    try:
        config = load_config(request.config_path)
        result = run_etl(config, parallel=request.parallel)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Migration failed: {str(e)}")

@app.get("/health")
async def health():
    return {"status": "healthy"}

# –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
class ETLApp:
    def __init__(self, root):
        self.root = root
        self.root.title("ETL Migration Tool (Big Data)")
        self.logger = get_logger()
        self.config = load_config()
        self.data_chunks = None
        self.client = None

        # –≠–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        tk.Label(root, text="ETL Migration Tool (Big Data)", font=("Arial", 16)).pack(pady=10)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Dask –∫–ª–∞—Å—Ç–µ—Ä–∞
        tk.Button(root, text="–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Dask –∫–ª–∞—Å—Ç–µ—Ä", command=self.init_dask).pack(pady=5)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        tk.Button(root, text="–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ (Excel)", command=self.load_data).pack(pady=5)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        tk.Button(root, text="–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", command=self.train_model).pack(pady=5)

        # –ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–∏
        tk.Button(root, text="–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é", command=self.run_migration).pack(pady=5)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        self.progress = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")
        self.progress.pack(pady=10)

        # –õ–æ–≥–∏
        self.log_area = scrolledtext.ScrolledText(root, width=80, height=20)
        self.log_area.pack(pady=10)

    def log(self, message):
        self.log_area.insert(tk.END, message + "\n")
        self.log_area.yview(tk.END)
        self.logger.info(message)

    def init_dask(self):
        try:
            self.client = Client(n_workers=4, threads_per_worker=2)
            self.log(f"Dask –∫–ª–∞—Å—Ç–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.client}")
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Dask: {str(e)}")

    def load_data(self):
        try:
            # –ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            self.data_chunks = dd.from_pandas(pd.read_excel("Book1.xlsx", engine='openpyxl'), npartitions=100)
            self.log(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (Dask): {len(self.data_chunks)} —Å—Ç—Ä–æ–∫, {len(self.data_chunks.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤.")
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {str(e)}")

    def train_model(self):
        threading.Thread(target=self._train_model, daemon=True).start()

    def _train_model(self):
        self.log("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        self.progress["value"] = 0
        train_model(self.data_chunks)
        self.log("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞.")
        self.progress["value"] = 100

    def run_migration(self):
        threading.Thread(target=self._run_migration, daemon=True).start()

    def _run_migration(self):
        if self.data_chunks is None:
            messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
            return
        self.log("–ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–∏...")
        self.progress["value"] = 0
        try:
            result = run_etl(self.config, self.data_chunks, parallel=True, client=self.client)
            self.log(f"–ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
            self.progress["value"] = 100
        except Exception as e:
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–ú–∏–≥—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)}")

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    mode = sys.argv[1] if len(sys.argv) > 1 else "gui"

    if mode == "train":
        train_model()
    elif mode == "migrate":
        config = load_config()
        run_etl(config)
    elif mode == "api":
        port = int(os.getenv("API_PORT", 8000))
        uvicorn.run(app, host="0.0.0.0", port=port)
    elif mode == "gui":
        root = tk.Tk()
        app = ETLApp(root)
        root.mainloop()
    else:
        print("Usage: python etl_project.py [train|migrate|api|gui]")